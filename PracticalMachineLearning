#   Load Library
    library(abind);
    library(arm);
    library(klaR)
    library(caret);
    library(kernlab);
    library(rattle);
    library(randomForest);
    library(rpart);

    
#   Enable Multi-Core Processing
    library(doParallel);
    cluster <- makeCluster(detectCores());
    registerDoParallel(cluster);

    
#   Data Processing
    set.seed(777);  # Set Random Seed
    dataset <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""));
    validation <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""));

    inTrain <- createDataPartition(y=dataset$classe, p=0.50, list=F);
    training <- dataset[inTrain, ];
    testing <- dataset[-inTrain, ];

    training <- subset(training, select = c(8:11, 37:49, 60:68, 84:86, 102, 
        113:124, 140, 151:160));  # Remove Unused Variables
    training[is.na(training)] <- 0;  # Replace NAs in Dataset with 0
    rm(dataset, inTrain);  # Remove Unused Datasets


#   Correlation Analysis
    correlation <- caret::findCorrelation(cor(training[, 1:52]), cutoff=0.80);
    correlationMatrix <- data.frame(cor(training[, 1:52]));
    names(training)[correlation];

    
#   Pre-Processing & Cross Validation Settings
    trControl <- trainControl(method="cv", number=7, verboseIter=F, preProcOptions="pca",
        allowParallel=T);


#   Model Development & Analysis
    rf <- train(classe~., data=training, method="rf", 
        trControl=trControl);  # Random Forest
    svmLinear <- train(classe~., data=training, method="svmLinear", 
        trControl=trControl);  # Support Vector Machine - Linear
    svmRadial <- train(classe~., data=training, method="svmRadial", 
        trControl=trControl);  # Support Vector Machine - Radial
    nnet <- train(classe~., data=training, method="nnet", 
        trControl=trControl, verbose=F);  # Neural Net
    bayesglm <- train(classe~., data=training, method="bayesglm", 
        trControl=trControl);  # Bayes Generalized Linear Model
    logitBoosted <- train(classe~., data=training, method="LogitBoost", 
        trControl=trControl);  # Logit Boosted Model
    rpart <- train(classe~., data=training, method="rpart", 
        trControl=trControl);  # Recursive Partitioning & Regression Trees
    
#   Prediction on the Test Set
    rfPred <- predict(rf, testing);
    svmLinearPred <- predict(svmLinear, testing);
    svmRadialPred <- predict(svmRadial, testing);
    nnetPred <- predict(nnet, testing);
    bayesglmPred <- predict(bayesglm, testing);
    logitBoostedPred <- predict(logitBoosted, testing);
    rpartPred <- predict(rpart, testing);

    
    testResults <- data.frame(cbind(as.character(testing$classe), as.character(rfPred),     
        as.character(svmLinearPred), as.character(svmRadialPred),
        as.character(nnetPred), as.character(bayesglmPred), 
        as.character(logitBoostedPred), as.character(rpartPred)));

    colnames(testResults) <- c("classe", "rfPred", "svmLinearPred", "svmRadialPred",
        "nnetPred", "bayesglmPred", "logitBoostedPred", "rpartPred");

    
#   Model Accuracy: Training Set
    testResults$rfPredRight[testResults$classe == testResults$rfPred] <- 1;
    testResults$rfPredRight[testResults$classe != testResults$rfPred] <- 0;
    
    testResults$svmLinearPredRight[testResults$classe == testResults$svmLinearPred] <- 1;
    testResults$svmLinearPredRight[testResults$classe != testResults$svmLinearPred] <- 0; 
    
    testResults$svmRadialPredRight[testResults$classe == testResults$svmRadialPred] <- 1;
    testResults$svmRadialPredRight[testResults$classe != testResults$svmRadialPred] <- 0;
    
    testResults$nnetPredRight[testResults$classe == testResults$nnetPred] <- 1;
    testResults$nnetPredRight[testResults$classe != testResults$nnetPred] <- 0;
    
    
    
    sum(testResults$rfPredRight)/dim(testResults)[1];
    sum(testResults$svmLinearPredRight)/dim(testResults)[1];
    sum(testResults$svmRadialPredRight)/dim(testResults)[1];
    
    
    modelName <- c(
        "Random Forest",
        "Support Vector Machine - Linear",        
        "Support Vector Machine - Radial",        
        "Neural Net",
        "Bayes Generalized Linear Model",        
        "Logit Boosted Model",        
        "Recursive Partitioning & Regression Trees"      
    );
    
    inSample1 <- c(
        max(rf$results$Accuracy),
        max(svmLinear$results$Accuracy),        
        max(svmRadial$results$Accuracy),        
        max(nnet$results$Accuracy),
        max(bayesglm$results$Accuracy),        
        max(logitBoosted$results$Accuracy),        
        max(rpart$results$Accuracy)       
    );
    
    
#   Prediction on the Validation Set
    rfPred <- predict(rf, validation);
    svmRadialPred <- predict(svmRadial, validation);
    nnetPred <- predict(nnet, validation);    
    svmLinearPred <- predict(svmLinear, validation);    
    bayesglmPred <- predict(bayesglm, validation);    
    LogitBoostedPred <- predict(LogitBoosted, validation);    
    rpartPred <- predict(rpart, validation);

    
    
    
    
    
    pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
            filename = paste0("problem_id_",i,".txt")
            write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
    }
    
    pml_write_files(rfPred);
    
